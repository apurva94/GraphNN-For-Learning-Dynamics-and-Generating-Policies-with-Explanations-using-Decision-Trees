{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ae795-ebc8-430a-b93a-263d472ab022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/stak/users/kokatea/hpc-share/anaconda3/envs/py37/lib/python3.7/site-packages/glfw/__init__.py:906: GLFWError: (65544) b'X11: Failed to open display localhost:14.0'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./swimmer3_td3_c/TD3_17\n"
     ]
    }
   ],
   "source": [
    "import dmc2gym\n",
    "import sys\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3.common.results_plotter import load_results\n",
    "\n",
    "def ts2xy(timesteps, xaxis):\n",
    "    \"\"\"\n",
    "    Decompose a timesteps variable to x ans ys\n",
    "\n",
    "    :param timesteps: (Pandas DataFrame) the input data\n",
    "    :param xaxis: (str) the axis for the x and y output\n",
    "        (can be X_TIMESTEPS='timesteps', X_EPISODES='episodes' or X_WALLTIME='walltime_hrs')\n",
    "    :return: (np.ndarray, np.ndarray) the x and y output\n",
    "    \"\"\"\n",
    "    if xaxis == X_TIMESTEPS:\n",
    "        x_var = np.cumsum(timesteps.l.values)\n",
    "        y_var = timesteps.r.values\n",
    "    elif xaxis == X_EPISODES:\n",
    "        x_var = np.arange(len(timesteps))\n",
    "        y_var = timesteps.r.values\n",
    "    elif xaxis == X_WALLTIME:\n",
    "        x_var = timesteps.t.values / 3600.\n",
    "        y_var = timesteps.r.values\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return x_var, y_var\n",
    "\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq, log_dir, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(\"Saving new best model at {} timesteps\".format(x[-1]))\n",
    "                    print(\"Saving new best model to {}.zip\".format(self.save_path))\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class ProgressBarCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    :param pbar: (tqdm.pbar) Progress bar object\n",
    "    \"\"\"\n",
    "    def __init__(self, pbar):\n",
    "        super(ProgressBarCallback, self).__init__()\n",
    "        self._pbar = pbar\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Update the progress bar:\n",
    "        self._pbar.n = self.num_timesteps\n",
    "        self._pbar.update(0)\n",
    "\n",
    "# this callback uses the 'with' block, allowing for correct initialisation and destruction\n",
    "class ProgressBarManager(object):\n",
    "    def __init__(self, total_timesteps): # init object with total timesteps\n",
    "        self.pbar = None\n",
    "        self.total_timesteps = total_timesteps\n",
    "        \n",
    "    def __enter__(self): # create the progress bar and callback, return the callback\n",
    "        self.pbar = tqdm(total=self.total_timesteps)\n",
    "            \n",
    "        return ProgressBarCallback(self.pbar)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb): # close the callback\n",
    "        self.pbar.n = self.total_timesteps\n",
    "        self.pbar.update(0)\n",
    "        self.pbar.close()\n",
    "\n",
    "auto_save_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir='td3_models')\n",
    "        \n",
    "env = dmc2gym.make(domain_name='myswimmer', task_name='swimmer', seed=1)\n",
    "from stable_baselines3 import PPO, TD3\n",
    "model = TD3(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./swimmer3_td3_c/\") #base sb3\n",
    "# with ProgressBarManager(10000) as progress_callback:\n",
    "model.learn(total_timesteps=110000, log_interval=10)\n",
    "model.save(\"swimmer3_td3_correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b95bbf-6ef6-447c-ab87-7ad32042b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, TD3\n",
    "model = TD3.load(\"swimmer3_td3_correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec99e0-33c7-4310-952e-8c9727ebbe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dmc2gym\n",
    "import sys\n",
    "import dm_control.suite\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "env_opt = dm_control.suite.load(domain_name='myswimmer', task_name='swimmer', task_kwargs={'random': 10})\n",
    "\n",
    "\n",
    "# env = dmc2gym.make(domain_name='myswimmer', task_name='swimmer', seed=1)\n",
    "\n",
    "def sanitise_action(action):\n",
    "    if action<-1:\n",
    "        action = -1\n",
    "    elif action > 1:\n",
    "        action = 1\n",
    "    return action\n",
    "\n",
    "def flatten_obs(obs):\n",
    "  obs_pieces = []\n",
    "  for v in obs.values():\n",
    "    flat = np.array([v]) if np.isscalar(v) else v.ravel()\n",
    "    obs_pieces.append(flat)\n",
    "  return np.concatenate(obs_pieces, axis=0)\n",
    "\n",
    "\n",
    "init_dataset = []\n",
    "init_targets = []\n",
    "#env_opt.reset()\n",
    "init_actions = []\n",
    "init_rewards = []\n",
    "\n",
    "time_step = env_opt.reset()\n",
    "obs = env_opt.observation_spec()\n",
    "\n",
    "while not time_step.last():\n",
    "    obs = flatten_obs(time_step.observation)\n",
    "    action, _ = model.predict(obs)\n",
    "    # action_opt = env_opt._task.get_optimal_action(env_opt._physics)\n",
    "    init_dataset.append(obs)\n",
    "    init_targets.append(1 if action[0]>0 else 0)\n",
    "    #action[0] = sanitise_action(action_opt)\n",
    "    # obs, reward, done, info = env.step(action[0])\n",
    "    # time_Step = env_opt.step(action[0])\n",
    "    time_step = env_opt.step(action)\n",
    "    reward=time_step.reward\n",
    "    obs = flatten_obs(time_step.observation)\n",
    "    # print(obs)\n",
    "    \n",
    "    init_actions.append(action)\n",
    "    # opt_actions.append(action_opt)\n",
    "    init_rewards.append(reward)\n",
    "    \n",
    "opt_dataset = []\n",
    "opt_targets = []\n",
    "#env_opt.reset()\n",
    "opt_actions = []\n",
    "opt_policy_actions = []\n",
    "opt_rewards = []\n",
    "\n",
    "env_opt = dm_control.suite.load(domain_name='myswimmer', task_name='swimmer', task_kwargs={'random': 10})\n",
    "\n",
    "time_step = env_opt.reset()\n",
    "obs = env_opt.observation_spec()\n",
    "\n",
    "while not time_step.last():\n",
    "    obs = flatten_obs(time_step.observation)\n",
    "    action, _ = model.predict(obs)\n",
    "    action_opt = env_opt._task.get_optimal_action(env_opt._physics)\n",
    "    opt_dataset.append(obs)\n",
    "    if using_optimal is True:\n",
    "        opt_targets.append(1 if action[0]>0 else 0)\n",
    "    action[0] = sanitise_action(action_opt)\n",
    "    time_step = env_opt.step(action)\n",
    "    reward=time_step.reward\n",
    "    obs = flatten_obs(time_step.observation)\n",
    "    # print(obs)\n",
    "    # dataset.append(obs)\n",
    "    \n",
    "    opt_actions.append(action)\n",
    "    # opt_actions.append(action_opt)\n",
    "    opt_rewards.append(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c401be-55ad-4273-8143-477e79757bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('activity_optactive_reward-100000', 'w') as csvfile:\n",
    "    writer=csv.writer(csvfile, delimiter=',')\n",
    "    writer.writerows(zip(init_dataset,init_actions, init_rewards, init_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f79f41-ba4c-462c-9241-38f380143ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('activity_optactive_reward2-100000', 'w') as csvfile:\n",
    "    writer=csv.writer(csvfile, delimiter=',')\n",
    "    writer.writerows(zip(opt_dataset,opt_actions, opt_rewards, opt_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeebad1-9ff6-4650-8545-040a98a38dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(random_state=0, max_depth=3)\n",
    "clf_init = clf.fit(init_dataset, init_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bed6fa-35b6-43a4-906f-920e462af8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "export_text(clf, feature_names=['1','2','3','4','5','6','7','8','9','10',\n",
    "                               '11','12','13','14','15','16','17','18','19','20',\n",
    "                               '21','22','23','24','25','26','27','28','29','30',\n",
    "                               '31','32','33','34','35','36','37','38'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb07a63-0e82-4da6-abba-098c83ea99a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)\n",
    "tree.plot_tree(clf)\n",
    "fig.savefig('initialtree-110000.png')\n",
    "# plt.savefig('initialtree.png',format='eps',bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b116936b-6e31-4e1f-ba48-9b65aa125ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state=0, max_depth=3)\n",
    "clf_opt = clf.fit(opt_dataset, opt_targets)\n",
    "export_text(clf, feature_names=['1','2','3','4','5','6','7','8','9','10',\n",
    "                               '11','12','13','14','15','16','17','18','19','20',\n",
    "                               '21','22','23','24','25','26','27','28','29','30',\n",
    "                               '31','32','33','34','35','36','37','38'])\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)\n",
    "tree.plot_tree(clf)\n",
    "fig.savefig('optimaltree-110000.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a7047-0516-41e8-bdb9-648e68627056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitarray as bt\n",
    "true_positive = 0\n",
    "false_positive = 0\n",
    "opt_classes = clf_opt.predict(opt_dataset)\n",
    "init_classes = clf_init.predict(opt_dataset)\n",
    "accuracy = sum(1 for x,y in zip(opt_classes,init_classes) if x == y) / len(init_classes)\n",
    "from sklearn.metrics import precision_recall_fscore_support as pr\n",
    "bPrecis, bRecall, bFscore, bSupport = pr(init_classes, opt_classes, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9f2bc4-3b3b-4539-aad1-9fbfaf8e8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56f336-097d-45c3-8c76-95710eea8f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"performance-110000.txt\", \"w\")\n",
    "file.write(\"Timesteps = 110000 \\nAccuracy = \" + str(accuracy) + \"\\n\" +\"Precision = \"+ str(bPrecis) + \"\\n\"+\"Recall = \"+ str(bRecall) )\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399bdfdd-c409-4397-b1f5-a8ed25b2de06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cbcce0-832f-487e-a0a3-95f3e1815fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# Define a uniform random policy.\n",
    "# import matplotlib\n",
    "# matplotlib.use('TkAgg')\n",
    "# import matplotlib.pyplot as plt\n",
    "# # actions =[]\n",
    "# width = 300\n",
    "# height = 300\n",
    "# video = np.zeros((1, height, 2 * width, 3), dtype=np.uint8)\n",
    "# max_frame =1000\n",
    "# j=0\n",
    "# done = False\n",
    "# print(env.observation_spec().values())\n",
    "# print(env.action_spec())\n",
    "# # # plt.show()\n",
    "# # while not time_step.last():\n",
    "# #   obs = flatten_obs(time_step.observation)\n",
    "# #   action, _ = model.predict(obs)\n",
    "# #   time_step = env.step(action)\n",
    "# #   reward=time_step.reward\n",
    "# #   video[0] = np.hstack([env.physics.render(300, 300, camera_id=0),\n",
    "# #                         env.physics.render(300, 300, camera_id=1)])\n",
    "# #     # print(time_step.reward, time_step.discount, time_step.observation)\n",
    "# # # for i in range(max_frame):\n",
    "# #   img = plt.imshow(video[0])\n",
    "# #   plt.pause(0.01)  # Need min display time > 0.0.\n",
    "# #   plt.draw()\n",
    "# #   j+=1\n",
    "# #\n",
    "# # actions=np.array(actions)\n",
    "# # print(actions.shape)\n",
    "# # i=0\n",
    "# ####################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
